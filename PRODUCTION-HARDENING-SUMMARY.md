# Production Hardening Implementation Summary
**Date**: 2026-01-24
**Context**: Post PR-8B.1 deployment hardening
**Status**: âœ… Complete (all 5 requirements implemented)

---

## Overview

After successful PR-8B.1 deployment (pool exhaustion fix + predictions cache), implemented 5 critical production hardening improvements to prevent regressions and ensure system reliability.

---

## âœ… 1. 24-Hour Monitoring with Clear Targets

### Implementation
**File**: `scripts/monitor-pool-health.ts`

### Metrics & Thresholds
| Metric | Target | Trigger | Action |
|--------|--------|---------|--------|
| MaxClients errors | 0 | >3 in 1 hour | Investigate query optimization or increase pool |
| Cache hit rate | >60% | <50% | Increase TTL from 30s to 60s |
| DB active connections | <45 | >45 for 5min+ | Optimize queries / investigate long transactions |
| Pool waiting queue | 0 | >5 requests | Check for connection leaks |

### Features
- 60-second interval monitoring (configurable)
- 24-hour default duration with historical stats
- Real-time alerts printed to console
- Final summary report with recommendations
- Tracks: MaxClients, cache hit rate, DB connections, pool stats

### Usage
```bash
# Production monitoring (24h)
npx tsx scripts/monitor-pool-health.ts

# Quick test (5min, 10s interval)
# Edit line 205: monitor.run(10, 300)
```

### Output Example
```
=== ALERTS ===
ðŸš¨ MaxClients errors detected: 5 (target: 0)
âš ï¸ Cache hit rate low: 45% (target: >60%, trigger: <50%)
âš ï¸ DB active connections high: 47/50 (trigger: >45)
```

---

## âœ… 2. Cache Hardening

### Implementation
**Files**:
- `src/utils/cache/singleFlight.ts` (NEW)
- `src/routes/prediction.routes.ts` (MODIFIED)

### Problem 1: Cache Key Collision Risk
**Before**:
```typescript
cacheKey = `${userId||'anon'}:${limit}`;
```
- Risk: Adding new params (date, filters, pagination) causes wrong cache hits
- Example: `anon:50` would match ANY request with limit=50, ignoring other params

**After**:
```typescript
function generatePredictionsCacheKey(queryParams: Record<string, any>): string {
  const sortedKeys = Object.keys(queryParams).sort();
  const normalized: Record<string, any> = {};

  for (const key of sortedKeys) {
    const value = queryParams[key];
    if (value !== undefined && value !== null) {
      normalized[key] = value;
    }
  }

  const paramsString = JSON.stringify(normalized);
  const hash = crypto.createHash('md5').update(paramsString).digest('hex').substring(0, 12);
  return `matched:${hash}`;
}
```
- âœ… All query params included in key
- âœ… Deterministic (sorted keys)
- âœ… Collision-resistant (MD5 hash)
- âœ… Future-proof (new params automatically included)

### Problem 2: Cache Stampede on Expiration
**Before**:
```typescript
// 20 concurrent requests all hit DB when cache expires
const cached = memoryCache.get('predictions', cacheKey);
if (!cached) {
  const predictions = await dbQuery(); // 20 queries!
}
```

**After - Single-Flight Pattern**:
```typescript
const response = await singleFlight.do(cacheKey, async () => {
  // Double-check cache (another request might have populated it)
  const doubleCheck = memoryCache.get('predictions', cacheKey);
  if (doubleCheck) return doubleCheck;

  // Only 1 DB query executes, others wait for result
  const predictions = await dbQuery();
  memoryCache.set('predictions', cacheKey, result);
  return result;
});
```

**Single-Flight Features**:
- Only 1 DB query per cache key at a time
- Concurrent requests wait for in-flight promise
- 10s timeout for stale requests
- Stats tracking: deduplicated vs executed
- Automatic cleanup on success/error

### Problem 3: Error Caching
**Before**: Errors could get cached, preventing retry

**After**:
```typescript
// Only cache successful responses
if (predictions && predictions.length >= 0) {
  memoryCache.set('predictions', cacheKey, result);
}
// Don't cache on error - next request will retry
```

### Performance Impact
- **Before**: 20 concurrent requests = 20 DB queries (pool exhaustion risk)
- **After**: 20 concurrent requests = 1 DB query (19 deduplicated)
- **Cache hit**: <1ms (in-memory)
- **Cache miss (single-flight)**: 1 DB query + N-1 waiting requests

---

## âœ… 3. Pool Max Increase Guard

### Implementation
**File**: `scripts/check-db-capacity.ts`

### Purpose
Prevent auto-increasing pool from 50â†’70 without verifying DB can handle it.

### Checks Performed
1. **Max connections**: Query `SHOW max_connections`
2. **Slow queries**: Count queries running >1s
3. **Lock waits**: Check for ungranted locks
4. **Idle-in-transaction**: Detect transaction leaks
5. **Connection utilization**: Calculate % of max_connections used

### Decision Tree
```
IF slow_queries > 5 OR lock_waits > 10 OR utilization > 70%
  â†’ âŒ Don't increase pool
  â†’ Fix queries / investigate contention first
ELSE
  â†’ âœ… Safe to increase pool
  â†’ Recommended max = min(db_max - 23, 80)
```

### Usage
```bash
# Check if safe to increase pool
npx tsx scripts/check-db-capacity.ts

# Exit code 0 = safe, 1 = not safe
echo $?
```

### Output Example
```
=== DB CAPACITY CHECK ===
Current pool max: 50

--- CONNECTION STATS ---
DB max_connections: 100
Current connections: 35
  Active: 15
  Idle: 20

--- PERFORMANCE METRICS ---
Slow queries (>1s): 2
Lock waits: 0

--- RECOMMENDATIONS ---
Recommended pool max: 77
Can increase pool? âœ… YES

âœ… All checks passed - safe to increase pool max
Suggested: DB_MAX_CONNECTIONS=77
```

### Warning Example
```
--- WARNINGS ---
âš ï¸ 12 slow queries detected (>1s) - Optimize queries before increasing pool
âš ï¸ Connection utilization high: 72.0% - Consider query optimization first

âš ï¸ FIX WARNINGS BEFORE INCREASING POOL MAX
```

---

## âœ… 4. Orchestrator Regression Guard

### Implementation
**Files**:
- `src/jobs/config/jobConfig.ts` (NEW)
- `src/jobs/matchMinute.job.ts` (MODIFIED)

### Problem
After PR-8B.* changes:
- Hardcoded timeouts and batch sizes scattered across jobs
- MAX_UPDATES_PER_TICK not env-configurable
- Risk of inconsistent null handling across orchestrator callers

### Solution 1: Centralized Job Config
**File**: `src/jobs/config/jobConfig.ts`

```typescript
export const JOB_CONFIG = {
  MATCH_MINUTE: {
    TIMEOUT_MS: parseInt(process.env.MATCH_MINUTE_TIMEOUT_MS || '120000'),
    BATCH_SIZE: parseInt(process.env.MATCH_MINUTE_BATCH_SIZE || '100'),
    MAX_UPDATES_PER_TICK: parseInt(process.env.MAX_UPDATES_PER_TICK || '50'),
    CRON: '* * * * *',
  },

  MATCH_WATCHDOG: {
    TIMEOUT_MS: parseInt(process.env.MATCH_WATCHDOG_TIMEOUT_MS || '300000'),
    CRON: '*/5 * * * *',
  },

  ORCHESTRATOR: {
    LOCK_TIMEOUT_MS: parseInt(process.env.ORCHESTRATOR_LOCK_TIMEOUT_MS || '10000'),
    MAX_RETRIES: parseInt(process.env.ORCHESTRATOR_MAX_RETRIES || '3'),
  },
};
```

**Features**:
- All timeouts and batch sizes in one file
- Environment variable overrides for production tuning
- Validation on module load
- Warnings for extreme values (e.g., MAX_UPDATES_PER_TICK > 100)
- `getConfigSummary()` helper for debugging

### Solution 2: Updated matchMinute.job.ts
**Before**:
```typescript
const MAX_UPDATES_PER_TICK = (() => {
  const envValue = parseInt(process.env.MAX_UPDATES_PER_TICK || '50', 10);
  return Number.isFinite(envValue) && envValue > 0 ? envValue : 50;
})();

async tick(batchSize: number = 100): Promise<void> {
  batchSize = Math.min(batchSize, MAX_UPDATES_PER_TICK);

  await jobRunner.run({
    timeoutMs: 120000, // hardcoded
  }, async () => { ... });
}
```

**After**:
```typescript
import { JOB_CONFIG } from './config/jobConfig';

async tick(batchSize: number = JOB_CONFIG.MATCH_MINUTE.BATCH_SIZE): Promise<void> {
  batchSize = Math.min(batchSize, JOB_CONFIG.MATCH_MINUTE.MAX_UPDATES_PER_TICK);

  await jobRunner.run({
    timeoutMs: JOB_CONFIG.MATCH_MINUTE.TIMEOUT_MS,
  }, async () => { ... });
}
```

### Solution 3: Null Handling Audit
**Verified**: All orchestrator callers are protected

**MatchOrchestrator.updateMatch()** (lines 105-111):
```typescript
const lockKey = LOCK_KEYS.matchUpdateLock(matchId);

// PR-8B.1: Skip update if matchId is invalid (lockKey === null)
if (lockKey === null) {
  logger.debug(`[MatchOrchestrator] Skipping update for invalid matchId: ${matchId}`);
  return { status: 'rejected_invalid', fieldsUpdated: [], reason: 'invalid_match_id' };
}
```

**Result**: All job files calling `matchOrchestrator.updateMatch()` are automatically protected - they receive `rejected_invalid` status if matchId is invalid. No additional null checks needed in caller code.

### Environment Variables
Production tuning without code changes:
```bash
# .env
MAX_UPDATES_PER_TICK=30                # Lower for high-load periods
MATCH_MINUTE_BATCH_SIZE=50             # Reduce batch size
MATCH_MINUTE_TIMEOUT_MS=90000          # Shorten timeout
ORCHESTRATOR_LOCK_TIMEOUT_MS=5000      # Faster lock timeout
```

---

## âœ… 5. data_completeness Migration Investigation

### Problem
Pre-existing error:
```
ERROR: column "data_completeness" does not exist
```

### Root Cause
Migration exists but hasn't been run yet.

### Files Involved
1. **Migration**: `src/database/migrations/add-half-statistics-persistence.ts`
   - Adds 4 columns to `ts_matches`:
     - `statistics_second_half` (JSONB)
     - `incidents_first_half` (JSONB)
     - `incidents_second_half` (JSONB)
     - `data_completeness` (JSONB) - tracking flag

2. **Migration Script**: `src/scripts/run-half-stats-migration.ts`
   - Idempotent (checks if columns exist first)
   - Creates GIN index on data_completeness
   - Verifies columns after migration

3. **Job**: `src/jobs/dataCompletenessValidator.job.ts`
   - âœ… Already defensive - checks if column exists before using it
   - Lines 131-146: Uses `information_schema.columns` to detect availability
   - Returns early if column doesn't exist (no error)

### Migration Schema
```sql
ALTER TABLE ts_matches
ADD COLUMN data_completeness JSONB DEFAULT '{"first_half": false, "second_half": false, "full_time": false}';

CREATE INDEX idx_ts_matches_data_completeness
ON ts_matches USING GIN (data_completeness);
```

### Solution
Run the migration script:
```bash
# Run migration
npx ts-node src/scripts/run-half-stats-migration.ts

# Verify columns
psql -U postgres -d goalgpt -c "SELECT column_name FROM information_schema.columns WHERE table_name='ts_matches' AND column_name='data_completeness';"
```

### Why Code Doesn't Break
The `dataCompletenessValidator.job.ts` is already defensive:
```typescript
// Line 139: Check if column exists
const hasDataCompleteness = columns.has('data_completeness');

// Lines 143-146: Skip if column doesn't exist
if (!hasDataCompleteness && !hasFirstHalfStats) {
  logger.debug('[DataCompleteness] Required columns not found, skipping');
  return [];
}

// Lines 162-167: Only query if column exists
if (hasDataCompleteness) {
  selectParts.push('data_completeness');
  incompleteConditions.push(`data_completeness->>'first_half' = 'false'`);
}
```

**Result**: No code changes needed - just run the migration.

---

## Production Checklist

### Before Increasing Pool Max 50â†’70
```bash
# 1. Run capacity check
npx tsx scripts/check-db-capacity.ts

# 2. If warnings exist:
#    - Optimize slow queries first
#    - Investigate lock contention
#    - Fix transaction leaks

# 3. If all green:
#    - Update .env: DB_MAX_CONNECTIONS=70
#    - Restart backend
#    - Monitor for 1 hour
```

### Monitoring Routine
```bash
# Start 24h monitoring
npx tsx scripts/monitor-pool-health.ts

# Check logs for alerts:
# - MaxClients errors (target: 0)
# - Cache hit rate (target: >60%)
# - DB active connections (trigger: >45)
```

### Environment Variables
```bash
# Orchestrator tuning (optional)
MAX_UPDATES_PER_TICK=50                # Default: 50
MATCH_MINUTE_BATCH_SIZE=100            # Default: 100
MATCH_MINUTE_TIMEOUT_MS=120000         # Default: 2 minutes

# Pool configuration
DB_MAX_CONNECTIONS=50                  # Current value
```

### Run data_completeness Migration
```bash
# One-time migration
npx ts-node src/scripts/run-half-stats-migration.ts
```

---

## Key Benefits

1. **Monitoring**: Clear metrics with actionable thresholds
2. **Cache**: Collision-resistant keys + stampede prevention
3. **Safety**: Can't increase pool without DB capacity check
4. **Maintainability**: Centralized config for all job parameters
5. **Reliability**: Migration ready, defensive code already in place

---

## Files Changed

### New Files
- `scripts/monitor-pool-health.ts` - 24h monitoring script
- `scripts/check-db-capacity.ts` - DB capacity check guard
- `src/utils/cache/singleFlight.ts` - Single-flight pattern implementation
- `src/jobs/config/jobConfig.ts` - Centralized job configuration

### Modified Files
- `src/routes/prediction.routes.ts` - Cache hardening with MD5 keys + single-flight
- `src/jobs/matchMinute.job.ts` - Use JOB_CONFIG for timeouts/batch sizes

### Migration Files (Existing, Ready to Run)
- `src/database/migrations/add-half-statistics-persistence.ts`
- `src/scripts/run-half-stats-migration.ts`

---

## Next Steps

1. âœ… All implementation complete
2. ðŸ”„ Run data_completeness migration: `npx ts-node src/scripts/run-half-stats-migration.ts`
3. ðŸ”„ Start 24h monitoring: `npx tsx scripts/monitor-pool-health.ts`
4. ðŸ”„ Monitor cache hit rate in production logs
5. ðŸ”„ If pool increase needed: run capacity check first

---

**Implementation Date**: 2026-01-24
**Status**: Production Ready âœ…
