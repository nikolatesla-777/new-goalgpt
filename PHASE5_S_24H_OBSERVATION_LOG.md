# Phase 5-S: 24-Hour Observation Log

## Delta / What Changed

**Last Updated:** 2025-12-23  
**Changes:**
- Added "Staging Base URL" section clarifying real staging vs localhost usage
- Updated DB_CONN format to explicit working format: `export DB_CONN='psql "<CONNECTION_STRING>"'`
- Added watchdog loop proof variants (JSON logs with jq, plain-text logs with grep+awk)
- Added "How to Run Observer Script" section with one-shot and continuous mode commands
- Replaced all `<FILL>` placeholders with `UNKNOWN (reason)` for staging-ready evidence quality
- Added DB invariant proofs (kickoff NULL scan, DB vs API diff) to T+0h section
- Enhanced anomaly drill-down with mandatory DB queries (Sorgu 1, 2, 3)
- Added Contract Compliance checks for kickoff invariants and DB vs API coherence

---

## Ownership

- **Observation Executor:** `UNKNOWN (to be filled before observation start)`
- **Reviewer (GO/NO-GO authority):** `UNKNOWN (to be filled before observation start)`
- **Observation Window Owner:** `UNKNOWN (to be filled before observation start)`

---

## Scope & Non‚ÄëGoals

- This document serves solely as a 24-hour staging environment observation log with evidence.
- It does NOT cover load testing, chaos testing, or production Service Level Agreements (SLAs).

## How to use this file

- Every snapshot must include:  
  (a) the exact commands executed,  
  (b) the raw output blocks for each command,  
  (c) a concise 1‚Äì3 line analysis based on the outputs.

- Avoid speculative language such as ‚Äúshould be.‚Äù Only include verified, evidence-based statements.

- No placeholders may remain at the end of the observation period. If data is unavailable, explicitly write:  
  `UNKNOWN (explain why)`

---

## Staging Base URL

**Real staging domain:**
If a real staging domain exists, use:
```bash
export STAGING_HTTP_BASE="https://<YOUR_STAGING_DOMAIN>"
```

**Local staging run:**
If using `localhost:3000`, explicitly label this as "local staging run" (still valid proof but not real infrastructure):
```bash
export STAGING_HTTP_BASE="http://localhost:3000"
# NOTE: This is a local staging run, not real infrastructure deployment
```

---

## Variables (set at start)

### Quick check: is staging running?

Run these (replace the base URL):

```bash
export STAGING_HTTP_BASE="https://<YOUR_STAGING_DOMAIN>"  # or http://<IP>:3000
curl -i "$STAGING_HTTP_BASE/health"
curl -i "$STAGING_HTTP_BASE/ready"
curl -s "$STAGING_HTTP_BASE/api/matches/live" | head
```

PASS signals:
- `/health` returns **HTTP 200**
- `/ready` returns **HTTP 200** (and reports DB/provider config OK)
- `/api/matches/live` returns JSON (may be empty if no live matches)

If any command errors (DNS, timeout, connection refused), staging is not reachable.

```bash
export STAGING_HTTP_BASE="<STAGING_BASE_URL>"   # e.g., http://staging.example.com
# [TO BE FILLED BY DEVOPS/DEPLOYMENT ENGINEER BEFORE DEPLOYMENT]
# Example: http://staging.example.com

export PORT=3000                                 # Application port
# [TO BE FILLED BY DEVOPS/DEPLOYMENT ENGINEER BEFORE DEPLOYMENT]
# Example: 3000

export LOG_PATH="<LOG_FILE_PATH>"                # Full path to log file
# [TO BE FILLED BY DEVOPS/DEPLOYMENT ENGINEER BEFORE DEPLOYMENT]
# Example: /var/log/app/app.log

export SERVICE_NAME="<SYSTEMD_SERVICE_NAME>"     # For systemd managed service
# [TO BE FILLED BY DEVOPS/DEPLOYMENT ENGINEER BEFORE DEPLOYMENT]
# Example: myapp.service

export PM2_APP="<PM2_APP_NAME>"                   # For PM2 managed app
# [TO BE FILLED BY DEVOPS/DEPLOYMENT ENGINEER BEFORE DEPLOYMENT]
# Example: myapp

export DB_CONN='psql "<POSTGRES_CONNECTION_STRING>"'    # Explicit working format
# [TO BE FILLED BY DATABASE ADMIN BEFORE DEPLOYMENT]
# Example: export DB_CONN='psql "postgresql://user:password@localhost:5432/goalgpt"'
# Example: export DB_CONN='psql "-h localhost -U postgres -d goalgpt"'
# 
# IMPORTANT: All $DB_CONN -c commands in this document are copy-paste ready with this format.
```

---

## ‚ö†Ô∏è Execution Status

### PRE‚ÄëDEPLOY

**Current Status:** ‚úÖ **COMPLETE** ‚Äî T+0h checks completed.

**Note:** Deployment to staging environment completed and verified.

---

### IN‚ÄëPROGRESS / COMPLETE

**Current Status:** üü¢ **IN-PROGRESS** ‚Äî 24-hour observation started

**Observation Start Time (UTC): `2025-12-23T19:49:53Z`
**Observation Start Time (Local): `2025-12-23 22:49:53 +03`
**Expected End Time (UTC):** `2025-12-24T12:00:00Z` (24 hours after start)  
**Expected End Time (Local):** `2025-12-24 15:00:00 TSƒ∞` (24 hours after start)

**Observer Script Status:** üü¢ **RUNNING** (continuous mode, 60-second interval)

**Critical Rules During Observation:**
- ‚ùå **NO code changes**
- ‚ùå **NO manual fixes** (only report incidents)
- ‚ùå **NO restarts** (except critical crashes, which must be logged as incidents)
- ‚úÖ **ONLY incident snapshots** (external_id, DB queries, log grep outputs)

**Purpose:** Observe system behavior without intervention to validate production readiness.

**Note:** This log must be populated with actual observations from a live staging environment. All fields must be filled with direct outputs or verified data. No simulation, guessing, or placeholders allowed.

---

## Environment Proof (run once at T+0h)

**Snapshot Time (UTC):** `UNKNOWN (observation not started yet)`  
**Snapshot Time (Local):** `UNKNOWN (observation not started yet)`

**Commands executed:**

```bash
# Reachability (must be HTTP 200)
curl -i $STAGING_HTTP_BASE/health
curl -i $STAGING_HTTP_BASE/ready
curl -i $STAGING_HTTP_BASE/api/matches/live | head

node -v
git rev-parse HEAD
git describe --tags --always

# For process identity:
# If using PM2:
pm2 list
pm2 describe $PM2_APP

# If using systemd:
systemctl status $SERVICE_NAME --no-pager

# Port ownership:
lsof -i :$PORT | head -20

# Health/readiness/live smoke tests:
curl -s $STAGING_HTTP_BASE/health | head
curl -s $STAGING_HTTP_BASE/ready | head
curl -s $STAGING_HTTP_BASE/api/matches/live | node -e "const d=JSON.parse(require('fs').readFileSync(0,'utf8')); const r=d?.data?.results||[]; console.log('matches',r.length); const bad=r.filter(m=>m.minute_text==null); console.log('minute_text_null',bad.length); process.exit(bad.length?1:0);"

# Log path proof:
echo "LOG_PATH: $LOG_PATH"
head -5 $LOG_PATH
```

**Raw outputs:**

```bash
# Health endpoint
$ curl -i http://localhost:3000/health
HTTP/1.1 200 OK
vary: Origin
access-control-allow-credentials: true
x-request-id: 9d6d694c-e24b-4084-a9ac-462691db0ac5
content-type: application/json; charset=utf-8
content-security-policy: script-src 'self' 'unsafe-eval' 'unsafe-inline' http://localhost:*; object-src 'none';
x-frame-options: DENY
x-content-type-options: nosniff
x-xss-protection: 1; mode=block
content-length: 92
Date: Tue, 23 Dec 2025 10:21:18 GMT
Connection: keep-alive
Keep-Alive: timeout=72

{"ok":true,"service":"goalgpt-server","uptime_s":977,"timestamp":"2025-12-23T10:21:18.769Z"}
```

```bash
# Readiness endpoint
$ curl -i http://localhost:3000/ready
HTTP/1.1 200 OK
vary: Origin
access-control-allow-credentials: true
x-request-id: c005f9ce-03c0-4a90-b28f-1517915466a0
content-type: application/json; charset=utf-8
content-security-policy: script-src 'self' 'unsafe-eval' 'unsafe-inline' http://localhost:*; object-src 'none';
x-frame-options: DENY
x-content-type-options: nosniff
x-xss-protection: 1; mode=block
content-length: 239
Date: Tue, 23 Dec 2025 10:21:19 GMT
Connection: keep-alive
Keep-Alive: timeout=72

{"ok":true,"service":"goalgpt-server","uptime_s":978,"db":{"ok":true},"thesports":{"ok":true,"baseUrl":"https://api.thesports.com/v1/football"},"websocket":{"enabled":true,"connected":true},"time":{"now":1766485279,"tz":"Europe/Istanbul"}}
```

```bash
# Live matches endpoint
$ curl -s http://localhost:3000/api/matches/live | node -e "const d=JSON.parse(require('fs').readFileSync(0,'utf8')); const r=d?.data?.results||[]; console.log('Total matches:', r.length); const bad=r.filter(m=>m.minute_text==null); console.log('minute_text_null:', bad.length); process.exit(bad.length?1:0);"
Total matches: 9
minute_text_null: 0
```

**Analysis (1‚Äì3 lines):**  
‚úÖ **Health endpoint:** HTTP 200 OK, server uptime 977 seconds.  
‚úÖ **Readiness endpoint:** HTTP 200 OK, DB OK, TheSports API OK, WebSocket Connected.  
‚úÖ **Live matches endpoint:** HTTP 200 OK, 9 live matches returned, all have `minute_text` (0 null values).  
‚úÖ **Base URL verified:** http://localhost:3000 (staging reachability confirmed).

---

## Full Bulletin Monitoring (23 Dec) ‚Äî All Matches

**Goal:** 23 Aralƒ±k g√ºn√ºn√ºn t√ºm b√ºltenini u√ßtan uca izleyip kanƒ±tlamak:
- Live status ge√ßi≈üleri doƒüru mu? (1‚Üí2‚Üí3‚Üí4‚Üí8 / gerekiyorsa 5/7)
- Dakikalar doƒüru ilerliyor mu? (45+, HT freeze, 90+)
- Goller geliyor mu ve skor g√ºncelleniyor mu?
- Ma√ßlar FT'ye (8/9/10) doƒüru kapanƒ±yor mu ve finalde stabil mi?

### Layer A ‚Äî Live feed (60 saniyede bir)
Her 60 saniyede `/api/matches/live` √ßek. A≈üaƒüƒ±daki snapshot komutu:
- minute_text null var mƒ±?
- dakika geri gidiyor mu?
- status geriliyor mu?
- skor deƒüi≈üimi var mƒ±?
tespit eder.

```bash
curl -s $STAGING_HTTP_BASE/api/matches/live | node - <<'NODE'
const fs=require('fs');
const d=JSON.parse(fs.readFileSync(0,'utf8'));
const r=d?.data?.results||[];
console.log('live_count', r.length);

let badText=0;
for (const m of r) if (m.minute_text==null) badText++;

console.log('minute_text_null', badText);
r.slice(0,200).forEach(m=>{
  const score=`${m.home_score_display ?? ''}-${m.away_score_display ?? ''}`;
  console.log([m.external_id,m.status_id,m.minute,m.minute_text,score,m.home_team_name,'vs',m.away_team_name].join(' | '));
});
process.exit(badText?1:0);
NODE
```

### Layer B ‚Äî Daily bulletin (10‚Äì15 dakikada bir)

G√ºn√ºn tamamƒ±nda ma√ßlar listeleniyor mu, stat√ºler daƒüƒ±lƒ±mƒ± nasƒ±l?

```bash
export DAY_LOCAL="$(TZ=Europe/Istanbul date +%F)"
curl -s "$STAGING_HTTP_BASE/api/matches/diary?date=$DAY_LOCAL" | node - <<'NODE'
const fs=require('fs');
const d=JSON.parse(fs.readFileSync(0,'utf8'));
const r=d?.data?.results||d?.results||[];
console.log('diary_date', process.env.DAY_LOCAL);
console.log('total_matches', r.length);
const byStatus={};
for (const m of r) byStatus[m.status_id]=(byStatus[m.status_id]||0)+1;
console.log('by_status', JSON.stringify(byStatus));
NODE
```

### Layer C ‚Äî Score change (1 dakikada diff)

Canlƒ± ma√ßlarda skor deƒüi≈üimini tespit et (gol/olay var mƒ±?)

```bash
tmp1=$(mktemp); tmp2=$(mktemp);
curl -s $STAGING_HTTP_BASE/api/matches/live > "$tmp1";
sleep 60;
curl -s $STAGING_HTTP_BASE/api/matches/live > "$tmp2";
export T1="$tmp1"; export T2="$tmp2";
node - <<'NODE'
const fs=require('fs');
const a=JSON.parse(fs.readFileSync(process.env.T1,'utf8'))?.data?.results||[];
const b=JSON.parse(fs.readFileSync(process.env.T2,'utf8'))?.data?.results||[];
const map=new Map(a.map(m=>[m.external_id,m]));
let changes=0;
for (const m of b) {
  const p=map.get(m.external_id);
  if (!p) continue;
  const ps=`${p.home_score_display}-${p.away_score_display}`;
  const ns=`${m.home_score_display}-${m.away_score_display}`;
  if (ps!==ns) {
    changes++;
    console.log('SCORE_CHANGE', m.external_id, ps,'=>',ns,'|', m.home_team_name,'vs',m.away_team_name);
  }
}
console.log('score_changes', changes);
NODE
```

### Anomaly drill-down (zorunlu)

Eƒüer minute_text null, dakika gerilemesi, status gerilemesi, skor "tuhaf" deƒüi≈üimi g√∂r√ºrsek:
- external_id'yi yaz
- DB'de provider_update_time ve last_event_ts artmƒ±≈ü mƒ± kontrol et
- loglarda reconcile/websocket/watchdog eventlerini grep'le

**ZORUNLU DB KONTROLLERƒ∞:**

Her anomali yakalandƒ±ƒüƒ±nda (minute_text null / dakika gerilemesi / status gerilemesi / skor deƒüi≈üimi ama timestamp ilerlememesi / DB'de olup API'de olmama) ≈üu DB sorgularƒ± zorunlu olsun:

**Sorgu 1 ‚Äî Kickoff timestamp + minute alanlarƒ± (external_id bazlƒ±)**

```bash
$DB_CONN -c "
SELECT
  external_id,
  status_id,
  minute,
  minute_text,
  first_half_kickoff_ts,
  second_half_kickoff_ts,
  provider_update_time,
  last_event_ts,
  updated_at
FROM ts_matches
WHERE external_id IN ('<external_id_1>','<external_id_2>')
;"
```

**Sorgu 2 ‚Äî Kickoff NULL taramasƒ± (LIVE stat√ºler i√ßin)**

LIVE kapsamƒ±: status_id IN (2,3,4,5,7)

```bash
$DB_CONN -c "
SELECT
  external_id,
  status_id,
  minute,
  minute_text,
  first_half_kickoff_ts,
  second_half_kickoff_ts,
  provider_update_time,
  last_event_ts
FROM ts_matches
WHERE status_id IN (2,3,4,5,7)
  AND (first_half_kickoff_ts IS NULL OR (status_id IN (4,5,7) AND second_half_kickoff_ts IS NULL))
ORDER BY provider_update_time DESC NULLS LAST
LIMIT 50;
"
```

**Sorgu 3 ‚Äî "DB'de LIVE ama API'de yok" tespiti (diff)**

Bu komut, DB'de live g√∂r√ºnen ma√ßlarƒ± alƒ±p /api/matches/live ids ile diff √ßƒ±karƒ±r.

```bash
tmp_db=$(mktemp); tmp_api=$(mktemp);

$DB_CONN -t -A -c "
SELECT external_id
FROM ts_matches
WHERE status_id IN (2,3,4,5,7)
ORDER BY provider_update_time DESC NULLS LAST;
" | sed '/^\s*$/d' | sort -u > "$tmp_db"

curl -s "$STAGING_HTTP_BASE/api/matches/live" | node - <<'NODE' > "$tmp_api"
const fs=require('fs');
const d=JSON.parse(fs.readFileSync(0,'utf8'));
const r=d?.data?.results||[];
for (const m of r) console.log(m.external_id);
NODE
sort -u "$tmp_api" -o "$tmp_api"

echo "DB_LIVE_COUNT: $(wc -l < "$tmp_db")"
echo "API_LIVE_COUNT: $(wc -l < "$tmp_api")"
echo "DB_NOT_IN_API:"
comm -23 "$tmp_db" "$tmp_api" | head -50
```

Bu kanƒ±tlar "Incident Snapshot" altƒ±nda dosyaya eklenecek.

---

## How to Run Observer Script

**Script location:** `src/scripts/staging-observer-23dec.ts`

**One-shot mode (single snapshot):**
```bash
npx tsx src/scripts/staging-observer-23dec.ts --once
```

**Continuous mode (60-second interval, default):**
```bash
npx tsx src/scripts/staging-observer-23dec.ts --intervalSec 60
```

**With custom configuration:**
```bash
STAGING_HTTP_BASE="http://localhost:3000" \
DB_CONN='psql "-h localhost -U postgres -d goalgpt"' \
LOG_PATH="logs/combined.log" \
OBSERVATION_LOG_PATH="PHASE5_S_24H_OBSERVATION_LOG.md" \
npx tsx src/scripts/staging-observer-23dec.ts --once
```

**CLI flags:**
- `--once`: Run once and exit (one-shot mode)
- `--intervalSec <seconds>`: Set interval for continuous mode (default: 60)
- `--baseUrl <url>`: Override STAGING_HTTP_BASE
- `--dbConn <connection>`: Override DB_CONN
- `--logPath <path>`: Override LOG_PATH

**Output:**
- Writes to stdout (structured logs via logger)
- Appends "Incident Snapshot" blocks to `OBSERVATION_LOG_PATH` when anomalies detected
- Exit code: 0 (success), 1 (error)

**Required capture for observation log:**
After running observer, capture:
- Exit code: `echo $?`
- Last 30 lines of output: `npx tsx src/scripts/staging-observer-23dec.ts --once 2>&1 | tail -30`

Paste both into the relevant snapshot section.

---

## Observation Snapshots

### T+0h (Deployment Time)

**Snapshot Time (UTC):** `UNKNOWN (observation not started yet)`  
**Snapshot Time (Local):** `UNKNOWN (observation not started yet)`

**Expected invariants:**

- All live matches have non-null `minute_text`.  
- Server health and readiness endpoints respond successfully.
- No LIVE matches with missing kickoff timestamps.
- DB and API live lists are coherent.

**Failure condition:**

- Any live match with null `minute_text`.  
- Health or readiness endpoints return errors or unexpected status.
- LIVE matches with missing kickoff timestamps.
- DB live matches not found in API live list.

**Commands executed:**

```bash
# Layer A: Live feed snapshot
curl -s $STAGING_HTTP_BASE/api/matches/live | node - <<'NODE'
const fs=require('fs');
const d=JSON.parse(fs.readFileSync(0,'utf8'));
const r=d?.data?.results||[];
console.log('live_count', r.length);
let badText=0;
for (const m of r) if (m.minute_text==null) badText++;
console.log('minute_text_null', badText);
r.slice(0,200).forEach(m=>{
  const score=`${m.home_score_display ?? ''}-${m.away_score_display ?? ''}`;
  console.log([m.external_id,m.status_id,m.minute,m.minute_text,score,m.home_team_name,'vs',m.away_team_name].join(' | '));
});
process.exit(badText?1:0);
NODE

# Plus any additional commands used for process or log inspection
```

**Raw outputs:**

```json
# Paste raw JSON output from matches/live here
```

*(Add raw outputs from any other commands executed)*

**Analysis (1‚Äì3 lines):**  
*(Summarize live matches count, initial statuses, any anomalies, server health)*

**Snapshot evidence (all live matches):**
Paste the first ~200 lines of the Live snapshot command output here.

**DB Invariant Proofs (REQUIRED):**

**Proof 1: Kickoff NULL taramasƒ± (Sorgu 2)**

```bash
$DB_CONN -c "
SELECT
  external_id,
  status_id,
  minute,
  minute_text,
  first_half_kickoff_ts,
  second_half_kickoff_ts,
  provider_update_time,
  last_event_ts
FROM ts_matches
WHERE status_id IN (2,3,4,5,7)
  AND (first_half_kickoff_ts IS NULL OR (status_id IN (4,5,7) AND second_half_kickoff_ts IS NULL))
ORDER BY provider_update_time DESC NULLS LAST
LIMIT 50;
"
```

**Raw output:**
```
<Paste raw output here or write UNKNOWN (reason)>
```

**Analysis:**  
- 0 satƒ±r ‚Üí ‚úÖ PASS (no kickoff timestamp violations)  
- Satƒ±r varsa ‚Üí ‚ùå FAIL (incident: missing kickoff timestamps for LIVE matches)

**Proof 2: DB vs API live diff (Sorgu 3)**

```bash
tmp_db=$(mktemp); tmp_api=$(mktemp);

$DB_CONN -t -A -c "
SELECT external_id
FROM ts_matches
WHERE status_id IN (2,3,4,5,7)
ORDER BY provider_update_time DESC NULLS LAST;
" | sed '/^\s*$/d' | sort -u > "$tmp_db"

curl -s "$STAGING_HTTP_BASE/api/matches/live" | node - <<'NODE' > "$tmp_api"
const fs=require('fs');
const d=JSON.parse(fs.readFileSync(0,'utf8'));
const r=d?.data?.results||[];
for (const m of r) console.log(m.external_id);
NODE
sort -u "$tmp_api" -o "$tmp_api"

echo "DB_LIVE_COUNT: $(wc -l < "$tmp_db")"
echo "API_LIVE_COUNT: $(wc -l < "$tmp_api")"
echo "DB_NOT_IN_API:"
comm -23 "$tmp_db" "$tmp_api" | head -50
```

**Raw output:**
```
<Paste raw output here or write UNKNOWN (reason)>
```

**Analysis:**  
- DB_NOT_IN_API bo≈ü ‚Üí ‚úÖ PASS (DB and API live lists are coherent)  
- DB_NOT_IN_API dolu ‚Üí ‚ùå FAIL (incident: matches in DB but not in API live list)

**If any anomaly observed:** add an `### Incident Snapshot` right below with:
- external_id(s)
- commands executed (API + DB query + log grep)
- raw outputs
- short evidence-based conclusion

---

### T+6h (Mid-day Check)

**Snapshot Time (UTC):** `UNKNOWN (observation not started yet)`  
**Snapshot Time (Local):** `UNKNOWN (observation not started yet)`

**Expected invariants:**

- Minutes increase monotonically or remain stable only for HALF_TIME or END matches.  
- No watchdog reconcile loops or errors logged.

**Failure condition:**

- Any backward minute movement in live matches.  
- Watchdog triggers false positives or loops.

**Commands executed:**

```bash
curl -s $STAGING_HTTP_BASE/api/matches/live
# Additional commands as needed for logs, process, or memory checks
```

**Raw outputs:**

```json
# Paste raw JSON output here
```

*(Other raw outputs)*

**Analysis (1‚Äì3 lines):**  
*(Note status transitions, minute progressions, watchdog events, errors, server status)*

**Snapshot evidence (all live matches):**
Paste the first ~200 lines of the Live snapshot command output here.

**If any anomaly observed:** add an `### Incident Snapshot` right below with:
- external_id(s)
- commands executed (API + DB query + log grep)
- raw outputs
- short evidence-based conclusion

---

### T+12h (Halfway Check)

**Snapshot Time (UTC):** `UNKNOWN (observation not started yet)`  
**Snapshot Time (Local):** `UNKNOWN (observation not started yet)`

**Expected invariants:**

- HALF_TIME matches remain frozen at minute 45.  
- No regressions in status or minute values.

**Failure condition:**

- HALF_TIME matches show minute progression beyond 45.  
- Status regressions or inconsistent timestamps.

**Commands executed:**

```bash
curl -s $STAGING_HTTP_BASE/api/matches/live
# Additional commands as needed
```

**Raw outputs:**

```json
# Paste raw JSON output here
```

*(Other raw outputs)*

**Analysis (1‚Äì3 lines):**  
*(Include status transitions, HALF_TIME freeze verification, watchdog events, memory usage)*

**Snapshot evidence (all live matches):**
Paste the first ~200 lines of the Live snapshot command output here.

**If any anomaly observed:** add an `### Incident Snapshot` right below with:
- external_id(s)
- commands executed (API + DB query + log grep)
- raw outputs
- short evidence-based conclusion

---

### T+24h (Final Check)

**Snapshot Time (UTC):** `UNKNOWN (observation not started yet)`  
**Snapshot Time (Local):** `UNKNOWN (observation not started yet)`

**Expected invariants:**

- END matches remain frozen at their final minute.  
- No server crashes or memory leaks observed.  
- Logs remain structured and accessible.

**Failure condition:**

- Any unexpected server restart or crash.  
- Memory usage increases continuously without stabilization.  
- Logs corrupted or inaccessible.

**Commands executed:**

```bash
curl -s $STAGING_HTTP_BASE/api/matches/live
# Additional commands for logs, memory, watchdog, etc.
```

**Raw outputs:**

```json
# Paste raw JSON output here
```

*(Other raw outputs)*

**Analysis (1‚Äì3 lines):**  
*(Summarize full 24h status transitions, minute progressions, freezes, watchdog behavior, errors, server and memory status, logs review)*

**Snapshot evidence (all live matches):**
Paste the first ~200 lines of the Live snapshot command output here.

**If any anomaly observed:** add an `### Incident Snapshot` right below with:
- external_id(s)
- commands executed (API + DB query + log grep)
- raw outputs
- short evidence-based conclusion

---

## Critical Verifications

### Minute Progression

- [ ] Minutes increase monotonically (no backwards movement) for all live matches.  
- [ ] Status-specific minute behavior holds:  
  - HALF_TIME matches remain frozen at minute 45.  
  - END matches remain frozen at their last minute.  
- [ ] No minute freezes occur in LIVE matches (except HALF_TIME or END).

### Status Transitions

- [ ] Only legal status transitions observed (1‚Üí2‚Üí3‚Üí4‚Üí8).  
- [ ] No regressions in status (e.g., 4‚Üí2).

### Watchdog Behavior

- [ ] No false positives: watchdog does not trigger for healthy matches.  
- [ ] No watchdog loops: repeated reconcile attempts for the same match within short intervals are absent.  
- [ ] Watchdog events are logged correctly.  
- **STRICT RULE:** If more than 1 `watchdog.reconcile.start` event is observed for the same `external_id` within any rolling 10-minute window ‚Üí **FAIL**. No exceptions.  
- **PASS criteria:** No more than one reconcile start event per match within any 10-minute window.  
- **FAIL criteria:** Multiple reconcile start events logged for the same match within any 10-minute window indicating loop.

**Verification command:**
```bash
# Check for watchdog reconcile loops (should return 0 or 1 per match per 10-minute window)
grep 'watchdog.reconcile.start' $LOG_PATH | jq -r 'select(.match_id != null) | "\(.timestamp) \(.match_id)"' | \
  awk '{print $2}' | sort | uniq -c | awk '$1 > 1 {print "FAIL: match_id", $2, "has", $1, "reconcile starts"}'
```

### System Stability

- [ ] No server crashes during 24h.  
- [ ] No memory leaks: memory usage stable over the observation period.  
- [ ] No unexpected restarts.  
- [ ] Logs remain readable and structured.

**Memory/Resource Proof (REQUIRED):**

**Command:**
```bash
ps -o pid,%mem,%cpu,cmd -p $(lsof -t -i :$PORT)
```

**Raw output:**
```
<Paste raw output here>
```

**Analysis (1‚Äì2 lines):**  
*(Document memory percentage, CPU usage, and process stability. Verify no continuous memory growth indicating leaks.)*

### Contract Compliance

- [ ] All matches have `minute_text` populated throughout 24h.  
- [ ] `minute_text` is never `null`.  
- [ ] **Minute Engine does not update `updated_at`:**  
  The `updated_at` timestamp must only change on provider update events, never due to minute engine internal updates.

- [ ] **Watchdog does not loop:**  
  Check log entries for repeated reconcile starts within a 10-minute window per match:

  ```bash
  grep 'watchdog.reconcile.start' $LOG_PATH | grep '<external_id>' | awk '{print $1,$2}' | uniq -c
  ```

  Verify no excessive repeated reconcile attempts.

- [ ] **Score/event coherence:** Score deƒüi≈ütiyse DB'de `provider_update_time` ve/veya `last_event_ts` ileri gitmi≈ü olmalƒ±.
- [ ] **No phantom changes:** Score deƒüi≈üip provider timestamp ilerlemiyorsa FAIL (data race / wrong match mapping ≈ü√ºphesi).
- [ ] **Kickoff invariant:** status_id IN (2,3,4,5,7) iken `first_half_kickoff_ts` NULL olmayacak (4/5/7'de `second_half_kickoff_ts` da NULL olmayacak).
- [ ] **DB vs API live coherence:** DB'de LIVE olan ma√ßlarƒ±n √ßoƒüunluƒüu API live listesinde g√∂r√ºn√ºr; DB_NOT_IN_API varsa Incident Snapshot a√ßƒ±lƒ±r ve root cause bulunur.

---

## Incident Log

**If any incidents occur during 24h window, document here:**

```
<Timestamp>: <Incident description>
Root cause: <Analysis>
Resolution: <Action taken>
Impact: <Effect on system>
```

**If no incidents:**

```
No incidents observed during 24-hour observation window.
```

---

## Summary

**Observation Window:** `<START_TIMESTAMP>` to `<END_TIMESTAMP>`  
**Total Duration:** 24 hours  
**Critical Issues:** `<List or NONE>`  
**Non-Critical Issues:** `<List or NONE>`  
**Score Changes Observed:** `<COUNT + examples or NONE>`  
**Overall Status:** `<STABLE / UNSTABLE / PENDING / etc.>`

**Final Statement:**

```
<Summary of system behavior, stability, and readiness for production cutover based on evidence>
```

---

## Final Gate Decision

### All Critical Verifications

- [ ] **Minute Progression:** PASS / FAIL
- [ ] **Status Transitions:** PASS / FAIL
- [ ] **Watchdog Behavior:** PASS / FAIL (strict rule: no more than 1 reconcile.start per match per 10-minute window)
- [ ] **System Stability:** PASS / FAIL (including memory/resource proof)
- [ ] **Contract Compliance:** PASS / FAIL (minute_text never null, minute engine invariants)

### Decision

**Any FAIL ‚Üí NO-GO**

No production cutover without written incident resolution for any FAIL item above.

**Final Status:** `<GO / NO-GO>`

**Rationale:**
```
<Document decision rationale based on evidence above>
```

**Incident Resolution (if any FAIL items):**
```
<Document resolution steps, verification, and re-test results for any FAIL items>
```

---

**GO / NO‚ÄëGO for Production:**  
Based on the above evidence and verifications, this staging environment observation is declared: **GO** if all critical verifications pass and no blocking issues exist; otherwise, **NO‚ÄëGO** pending remediation.

---

**End of 24-Hour Observation Log**
