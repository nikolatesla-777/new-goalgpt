# Stream-Based CSV Import Architecture

## ğŸ—ï¸ Mimari Genel BakÄ±ÅŸ

Bu modÃ¼l, bÃ¼yÃ¼k CSV dosyalarÄ±nÄ± verimli bir ÅŸekilde import etmek iÃ§in **Node.js Stream API** kullanÄ±r. TÃ¼m dosyayÄ± RAM'e yÃ¼klemeden, parÃ§a parÃ§a (chunk) iÅŸleyerek milyonlarca satÄ±rlÄ±k dosyalarÄ± bile iÅŸleyebilir.

## ğŸ“ ModÃ¼ler YapÄ±

### 1. **Data Transformers** (`utils/data-transformers.ts`)
- **Sorumluluk**: CSV verilerini database formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rme
- **Fonksiyonlar**:
  - `transformValue()` - Tek bir deÄŸeri dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
  - `transformRecord()` - TÃ¼m kaydÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
  - JSONB, UUID, Boolean, Numeric dÃ¶nÃ¼ÅŸÃ¼mleri

### 2. **Validators** (`utils/validators.ts`)
- **Sorumluluk**: Veri validasyonu ve duplicate kontrolÃ¼
- **Fonksiyonlar**:
  - `removeDuplicateIds()` - ID bazlÄ± duplicate kontrolÃ¼
  - `removeDuplicateEmails()` - Email bazlÄ± duplicate kontrolÃ¼
  - `filterValidForeignKeys()` - Foreign key validasyonu
  - `validateBatch()` - Batch seviyesinde validasyon

### 3. **Database Helpers** (`utils/database-helpers.ts`)
- **Sorumluluk**: Database iÅŸlemleri ve schema sorgularÄ±
- **Fonksiyonlar**:
  - `getTableColumns()` - Tablo kolonlarÄ±nÄ± sorgular
  - `getValidCustomerUserIds()` - GeÃ§erli user ID'lerini getirir
  - `executeBatchInsert()` - Batch insert iÅŸlemi
  - `truncateTable()` - Tabloyu temizler

### 4. **CSV Stream Processor** (`import-csv.ts`)
- **Sorumluluk**: Stream processing ve batch yÃ¶netimi
- **Ã–zellikler**:
  - Stream-based file reading
  - Batch processing (1000 kayÄ±t/batch)
  - Progress tracking
  - Error handling

## ğŸ”„ Ä°ÅŸlem AkÄ±ÅŸÄ±

```
CSV File
    â†“
[File Stream] â†’ [CSV Parser] â†’ [Record Processor] â†’ [Batch Accumulator]
                                                          â†“
                                                    [Batch Size = 1000]
                                                          â†“
                                                    [Validate & Transform]
                                                          â†“
                                                    [Database Insert]
```

## ğŸ’¡ Temel Prensipler

### 1. **Single Responsibility Principle**
Her modÃ¼l tek bir sorumluluÄŸa sahip:
- Transformers â†’ Sadece veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- Validators â†’ Sadece validasyon
- Helpers â†’ Sadece database iÅŸlemleri

### 2. **Separation of Concerns**
- Ä°ÅŸ mantÄ±ÄŸÄ± (business logic) ayrÄ±
- Veri eriÅŸimi (data access) ayrÄ±
- Stream processing ayrÄ±

### 3. **Memory Efficiency**
- TÃ¼m dosya RAM'e yÃ¼klenmez
- Chunk'lar halinde iÅŸlenir
- Batch'ler halinde database'e yazÄ±lÄ±r

## ğŸš€ KullanÄ±m

```typescript
// Otomatik import (tÃ¼m dosyalar)
npm run import-csv

// Programatik kullanÄ±m
import { importCSVFile } from './import-csv';

await importCSVFile(
  '/path/to/file.csv',
  'table_name',
  ['skip_column1', 'skip_column2']
);
```

## ğŸ“Š Performans

### Ã–nceki YÃ¶ntem (readFileSync)
- âŒ TÃ¼m dosya RAM'e yÃ¼klenir
- âŒ 500MB+ dosyalar iÃ§in "Out of Memory" hatasÄ±
- âŒ BÃ¼yÃ¼k dosyalar iÅŸlenemez

### Yeni YÃ¶ntem (Stream)
- âœ… Dosya parÃ§a parÃ§a okunur
- âœ… 10GB+ dosyalar iÅŸlenebilir
- âœ… DÃ¼ÅŸÃ¼k RAM kullanÄ±mÄ±
- âœ… Batch processing ile verimli insert

## ğŸ”§ KonfigÃ¼rasyon

```typescript
const BATCH_SIZE = 1000; // Her batch'te 1000 kayÄ±t
const MAX_FILE_SIZE = 10 * 1024 * 1024 * 1024; // 10GB limit
```

## ğŸ“ Notlar

- Stream processing sÄ±rasÄ±nda her kayÄ±t tek tek iÅŸlenir
- Batch'ler 1000 kayÄ±t dolduÄŸunda otomatik olarak database'e yazÄ±lÄ±r
- Duplicate kontrolÃ¼ her kayÄ±t iÃ§in yapÄ±lÄ±r (Set kullanarak)
- Foreign key validasyonu sadece gerekli tablolar iÃ§in yapÄ±lÄ±r

## ğŸ¯ Avantajlar

1. **Ã–lÃ§eklenebilirlik**: Milyonlarca satÄ±r iÅŸlenebilir
2. **Bellek VerimliliÄŸi**: DÃ¼ÅŸÃ¼k RAM kullanÄ±mÄ±
3. **Hata ToleransÄ±**: Bir kayÄ±t hata verse bile devam eder
4. **ModÃ¼lerlik**: Kolay test edilebilir ve bakÄ±m yapÄ±labilir
5. **Temiz Kod**: Her modÃ¼l tek sorumluluÄŸa sahip

