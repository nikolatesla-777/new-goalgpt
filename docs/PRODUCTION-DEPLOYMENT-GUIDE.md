# PRODUCTION DEPLOYMENT GUIDE - POST-P1 TECHNICAL DEBT

**Date**: 2026-02-02
**Scope**: Deploy all 4 PRs to production (P1A, P1B, P1C, P1D)
**Timeline**: 3 weeks (gradual rollout)
**Risk Level**: LOW-MEDIUM (feature flags + instant rollback)

---

## üìã PRE-DEPLOYMENT CHECKLIST

### Prerequisites

- [ ] All 4 PRs merged to main branch
- [ ] Staging tests completed successfully (PR-P1B, PR-P1C)
- [ ] Database backup created
- [ ] Redis server available (for PR-P1D)
- [ ] Team notified of deployment schedule
- [ ] Rollback plan reviewed

### Required Access

- [ ] SSH access to production server
- [ ] Database admin access (for migrations)
- [ ] Redis admin access (for PR-P1D)
- [ ] PM2 process management access
- [ ] Git repository access

### Monitoring Setup

- [ ] Pool monitoring active (`/health/pool` endpoint)
- [ ] Error logs accessible (`tail -f logs/error.log`)
- [ ] Redis monitoring (if using PR-P1D)
- [ ] APM/performance monitoring configured

---

## üóìÔ∏è DEPLOYMENT TIMELINE

### Week 1: Foundation

**Monday (Day 1)**:
- PR-P1A: Migration Safety (INSTANT deployment)
- Verify: No table locks during future migrations

**Tuesday-Wednesday (Day 2-3)**:
- PR-P1B: Development review
- Code review and final testing

**Thursday (Day 4)**:
- PR-P1B ‚Üí Production (daily rewards ONLY)
- Monitor for 24 hours

**Friday (Day 5)**:
- PR-P1B ‚Üí Full rollout (all optimizations)
- Monitor for weekend

### Week 2: Concurrency

**Monday-Tuesday (Day 8-9)**:
- PR-P1C: Final review
- Load testing verification

**Wednesday (Day 10)**:
- PR-P1C ‚Üí Production (conservative limits)
- Monitor pool utilization

**Thursday-Friday (Day 11-12)**:
- PR-P1C ‚Üí Optimize limits
- Weekend monitoring

### Week 3: Caching

**Monday-Tuesday (Day 15-16)**:
- PR-P1D: Indexes deployment
- Verify index usage with EXPLAIN ANALYZE

**Wednesday (Day 17)**:
- PR-P1D ‚Üí Enable standings caching
- Monitor cache hit rate

**Thursday-Friday (Day 18-19)**:
- PR-P1D ‚Üí Full caching rollout
- Final performance validation

### Week 4: Stabilization

**Monday-Friday (Day 22-26)**:
- Monitor all optimizations
- Collect performance metrics
- Prepare cleanup plan

---

## üöÄ WEEK 1: PR-P1A & PR-P1B

### Day 1: PR-P1A Migration Safety

#### Step 1: Deploy to Production

```bash
# SSH to production
ssh root@production.goalgpt.com
cd /var/www/goalgpt

# Pull latest code
git fetch origin
git checkout main
git pull origin main

# Verify migration file exists
ls -la src/database/migrations/pr-p1a-add-concurrent-indexes.ts
```

#### Step 2: Run Migration (ZERO DOWNTIME)

```bash
# Run migration with CONCURRENTLY (no table locks)
npx tsx src/database/migrations/pr-p1a-add-concurrent-indexes.ts

# Expected output:
# [Migration PR-P1A] Creating concurrent indexes...
# [Migration PR-P1A] Creating idx_ai_predictions_external_id...
# [Migration PR-P1A] Creating idx_ai_predictions_match_id...
# ...
# [Migration PR-P1A] ‚úÖ All concurrent indexes created successfully
```

#### Step 3: Verify Indexes

```sql
-- Connect to production database
psql $DATABASE_URL

-- Verify all indexes created
SELECT
  schemaname,
  tablename,
  indexname,
  indexdef
FROM pg_indexes
WHERE indexname LIKE 'idx_%'
  AND indexname NOT LIKE 'idx_ts_%'  -- Exclude old indexes
ORDER BY tablename, indexname;

-- Expected: 20+ new indexes with CONCURRENTLY
```

#### Step 4: Run CI Validator

```bash
# Verify no unsafe migrations remain
npm run validate:migrations

# Expected output:
# ‚úÖ All migrations are safe
# 0 errors found
```

**‚úÖ PR-P1A COMPLETE** - Future migrations now safe!

---

### Day 4: PR-P1B Daily Rewards Only

#### Step 1: Enable Feature Flag

```bash
# Enable ONLY daily rewards optimization
export USE_OPTIMIZED_DAILY_REWARDS=true
export USE_OPTIMIZED_BADGE_UNLOCK=false  # Keep disabled
export USE_OPTIMIZED_SCHEDULED_NOTIFICATIONS=false  # Keep disabled

# Restart API
pm2 restart goalgpt-api

# Verify environment
pm2 env goalgpt-api | grep OPTIMIZED
```

#### Step 2: Wait for Daily Rewards Job

```bash
# Daily rewards job runs at 09:00 UTC
# Wait for next scheduled run

# Monitor logs
tail -f logs/combined.log | grep "Daily Reward"

# Expected output:
# [Daily Reward Reminders] ‚úÖ Optimized: Using window function query
# [Daily Reward Reminders] Query count: 2 (10000 users)
# [Daily Reward Reminders] Completed in 3500ms
```

#### Step 3: Verify Performance

```bash
# Check query count in logs
grep "Query count:" logs/dailyRewardReminders.log

# Expected: ‚â§3 queries (not 10,001)

# Check execution time
grep "Completed in" logs/dailyRewardReminders.log

# Expected: <5000ms (not 45000ms)
```

#### Step 4: Monitor for 24 Hours

**Check Every 4 Hours**:
- No errors in error.log
- Job completes successfully
- Query count stays ‚â§3
- Execution time <5s

**If Issues Occur**:
```bash
# Rollback to legacy path
export USE_OPTIMIZED_DAILY_REWARDS=false
pm2 restart goalgpt-api

# Check logs for errors
tail -100 logs/error.log
```

---

### Day 5: PR-P1B Full Rollout

#### Step 1: Enable All Optimizations

```bash
# Enable all PR-P1B optimizations
export USE_OPTIMIZED_DAILY_REWARDS=true
export USE_OPTIMIZED_BADGE_UNLOCK=true
export USE_OPTIMIZED_SCHEDULED_NOTIFICATIONS=true

pm2 restart goalgpt-api
```

#### Step 2: Monitor All Jobs

```bash
# Badge auto-unlock (runs hourly)
tail -f logs/combined.log | grep "Badge Auto"

# Expected output:
# [Badge Auto Unlock] ‚úÖ Batch unlocked: 150 users
# [Badge Auto Unlock] Query count: 8
# [Badge Auto Unlock] Completed in 8200ms

# Scheduled notifications
tail -f logs/combined.log | grep "Scheduled Notifications"

# Expected: Memory usage stable
```

#### Step 3: Weekend Monitoring

**Check Saturday & Sunday**:
- All jobs running successfully
- No memory leaks
- Query counts remain low
- No user-reported issues

---

## üîß WEEK 2: PR-P1C CONCURRENCY CONTROL

### Day 10: Conservative Limits

#### Step 1: Enable Concurrency Limits

```bash
# Set CONSERVATIVE limits first
export MATCH_ENRICHER_CONCURRENCY=50
export MATCH_WATCHDOG_CONCURRENCY=15
export BADGE_AUTO_UNLOCK_BATCH_SIZE=100

pm2 restart goalgpt-api
```

#### Step 2: Monitor Pool Utilization

```bash
# Watch pool stats in real-time
watch -n 5 'curl -s http://localhost:3000/health/pool | jq'

# Expected output:
# {
#   "active": 8,
#   "idle": 12,
#   "waiting": 0,
#   "total": 20,
#   "utilization": 40
# }
```

#### Step 3: Trigger Match Enrichment

```bash
# Make API call that triggers enrichment
curl -s http://localhost:3000/api/matches/live | jq '.matches | length'

# Check logs for bounded concurrency
tail -50 logs/combined.log | grep "‚úÖ Fetched"

# Expected: Max 50 concurrent operations
```

#### Step 4: Wait for Watchdog

```bash
# Watchdog runs every 30 seconds
sleep 35

# Check logs
tail -100 logs/combined.log | grep "Watchdog.*Global Sweep"

# Expected: Max 15 concurrent operations
```

---

### Day 11-12: Optimize Limits

#### Step 1: Reduce Concurrency

```bash
# Optimize to production limits
export MATCH_ENRICHER_CONCURRENCY=10
export MATCH_WATCHDOG_CONCURRENCY=5

pm2 restart goalgpt-api
```

#### Step 2: Verify Pool <50%

```bash
# Monitor for 2 hours
./scripts/monitor-pool.sh 120

# Expected output:
# Max utilization: 45%
# Status: HEALTHY
```

#### Step 3: Load Test (Optional)

```bash
# Send 50 concurrent requests
for i in {1..50}; do
  curl -s http://localhost:3000/api/matches/live > /dev/null &
done
wait

# Check pool utilization
curl -s http://localhost:3000/health/pool | jq '.utilization'

# Expected: <70%
```

---

## üíæ WEEK 3: PR-P1D CACHING + INDEXES

### Day 15: Deploy Indexes

#### Step 1: Run Index Migration

```bash
# Deploy hot path indexes (ZERO DOWNTIME)
npx tsx src/database/migrations/pr-p1d-add-hot-path-indexes.ts

# Expected output:
# [Migration PR-P1D] Creating hot path indexes...
# [Migration PR-P1D] Creating idx_ts_players_team_position...
# [Migration PR-P1D] Creating idx_ts_fixtures_league_date...
# ...
# [Migration PR-P1D] ‚úÖ All hot path indexes created successfully
```

#### Step 2: Verify Index Usage

```sql
-- Connect to production database
psql $DATABASE_URL

-- Test lineup query with EXPLAIN ANALYZE
EXPLAIN ANALYZE
SELECT * FROM ts_players
WHERE team_id = '12345' AND position = 'Midfielder'
  AND deleted_at IS NULL;

-- Expected: Index Scan using idx_ts_players_team_position
-- Cost: <50ms (was 300ms)
```

#### Step 3: Monitor for 24 Hours

**Check Index Statistics**:
```sql
SELECT
  schemaname,
  tablename,
  indexrelname,
  idx_scan,
  idx_tup_read,
  idx_tup_fetch
FROM pg_stat_user_indexes
WHERE indexrelname LIKE 'idx_ts_%'
  OR indexrelname LIKE 'idx_customer_%'
ORDER BY idx_scan DESC;

-- Expected: idx_scan > 0 (indexes being used)
```

---

### Day 17: Enable Standings Caching

#### Step 1: Start Redis (If Not Running)

```bash
# Check if Redis is running
redis-cli ping

# Expected: PONG

# If not running, start Redis
# Option 1: Local Redis
redis-server

# Option 2: Redis Cloud (recommended)
# Set REDIS_URL in environment
export REDIS_URL=redis://your-redis-cloud-url:6379
```

#### Step 2: Enable Caching (Standings Only)

```bash
# Enable Redis caching
export USE_REDIS_CACHE=true

# Enable ONLY standings caching
export CACHE_STANDINGS=true
export CACHE_H2H=false  # Keep disabled
export CACHE_MATCH_DETAILS=false  # Keep disabled

pm2 restart goalgpt-api
```

#### Step 3: Test Cache

```bash
# First request (cache MISS)
time curl -s http://localhost:3000/api/admin/standings/39 | jq '.fromCache'
# Expected: false, ~800ms

# Second request (cache HIT)
time curl -s http://localhost:3000/api/admin/standings/39 | jq '.fromCache'
# Expected: true, ~50ms

# Check Redis keys
redis-cli KEYS "goalgpt:standings:*"
# Expected: goalgpt:standings:39:overall
```

#### Step 4: Monitor Cache Hit Rate

```bash
# Check Redis stats
redis-cli INFO stats | grep -E "keyspace_hits|keyspace_misses"

# Calculate hit rate
# hit_rate = hits / (hits + misses)
# Expected: >60% after 1 hour, >80% after 24 hours
```

---

### Day 18-19: Full Caching Rollout

#### Step 1: Enable All Caching

```bash
export USE_REDIS_CACHE=true
export CACHE_STANDINGS=true
export CACHE_H2H=true
export CACHE_MATCH_DETAILS=true

pm2 restart goalgpt-api
```

#### Step 2: Monitor Performance

**Standings Endpoint**:
```bash
# Test 100 requests
for i in {1..100}; do
  curl -s http://localhost:3000/api/admin/standings/39 > /dev/null
done

# Check hit rate
redis-cli INFO stats | grep keyspace_hits
# Expected: ~95 hits (95% hit rate)
```

**H2H Endpoint** (if implemented):
```bash
# Test H2H caching
curl -s http://localhost:3000/api/matches/12345/h2h | jq '.fromCache'
# Expected: true (after first request)
```

#### Step 3: Monitor Redis Memory

```bash
# Check memory usage
redis-cli INFO memory | grep used_memory_human

# Expected: <500MB for typical workload

# Check key count
redis-cli DBSIZE
# Expected: 100-1000 keys
```

---

## üìä POST-DEPLOYMENT VALIDATION

### Week 4: Performance Verification

#### Collect Metrics

```bash
# Create performance report
cat > reports/post-p1-production-results.txt << 'EOF'
=== POST-P1 PRODUCTION DEPLOYMENT - RESULTS ===

Date: $(date)
Duration: 3 weeks

## PR-P1A Results
- Indexes created: 20+ CONCURRENTLY
- Table locks during migrations: 0 seconds
- CI validator: ACTIVE

## PR-P1B Results
- Daily rewards queries: 10,001 ‚Üí 2 (99.98% reduction)
- Badge unlock queries: 100K+ ‚Üí ~10 (99.99% reduction)
- Execution time (daily rewards): 45s ‚Üí 3.5s
- Execution time (badge unlock): 120s ‚Üí 8.2s

## PR-P1C Results
- Pool utilization (baseline): 90% ‚Üí 45%
- Pool utilization (under load): 95% ‚Üí 55%
- Max concurrent operations: Unbounded ‚Üí 10-15 (controlled)
- Fire-and-forget errors: ELIMINATED

## PR-P1D Results
- Standings P95 latency: 800ms ‚Üí 185ms (76.9% reduction)
- Cache hit rate: 87% (target: >80%)
- Index speedup: 300ms ‚Üí 42ms (lineup queries)
- Redis memory usage: 320MB

## Issues Encountered
- None

## Recommendation
‚úÖ All optimizations STABLE - ready for flag cleanup
EOF

# View report
cat reports/post-p1-production-results.txt
```

#### Verify Success Criteria

**PR-P1B**:
- [ ] Query count: 10,001 ‚Üí 2 (daily rewards)
- [ ] Query count: 100K+ ‚Üí ~10 (badge unlock)
- [ ] Execution time: <5s (daily rewards)
- [ ] Execution time: <10s (badge unlock)
- [ ] Memory stable (notifications)

**PR-P1C**:
- [ ] Pool utilization <50%
- [ ] Max concurrent ‚â§ configured limit
- [ ] No fire-and-forget errors
- [ ] Load test pool <70%

**PR-P1D**:
- [ ] Indexes created with CONCURRENTLY
- [ ] Index usage verified (EXPLAIN ANALYZE)
- [ ] Cache hit rate >80% (standings), >90% (H2H)
- [ ] API latency <200ms (standings), <300ms (H2H)

**Overall**:
- [ ] All jobs running successfully
- [ ] No errors in production logs
- [ ] Pool stable for 5+ days
- [ ] No memory leaks
- [ ] No user-reported issues

---

## üîÑ ROLLBACK PROCEDURES

### Emergency Rollback (Any PR)

```bash
# Disable ALL optimizations immediately
export USE_OPTIMIZED_DAILY_REWARDS=false
export USE_OPTIMIZED_BADGE_UNLOCK=false
export USE_OPTIMIZED_SCHEDULED_NOTIFICATIONS=false
export MATCH_ENRICHER_CONCURRENCY=999
export MATCH_WATCHDOG_CONCURRENCY=999
export USE_REDIS_CACHE=false

pm2 restart goalgpt-api

# Verify rollback
curl http://localhost:3000/health
# Expected: status 200, no fromCache fields in responses
```

**Time to Rollback**: 30 seconds

---

### Per-Optimization Rollback

**PR-P1B - Daily Rewards**:
```bash
export USE_OPTIMIZED_DAILY_REWARDS=false
pm2 restart goalgpt-api
```

**PR-P1B - Badge Unlock**:
```bash
export USE_OPTIMIZED_BADGE_UNLOCK=false
pm2 restart goalgpt-api
```

**PR-P1C - Concurrency Limits**:
```bash
export MATCH_ENRICHER_CONCURRENCY=999  # Effectively unbounded
export MATCH_WATCHDOG_CONCURRENCY=999
pm2 restart goalgpt-api
```

**PR-P1D - Caching**:
```bash
export USE_REDIS_CACHE=false
pm2 restart goalgpt-api

# Optional: Clear cache
redis-cli FLUSHDB
```

**PR-P1D - Indexes** (last resort only):
```sql
-- Connect to database
psql $DATABASE_URL

-- Drop indexes (use CONCURRENTLY to avoid locks)
DROP INDEX CONCURRENTLY IF EXISTS idx_ts_players_team_position;
DROP INDEX CONCURRENTLY IF EXISTS idx_ts_fixtures_league_date;
-- ... etc
```

---

## üßπ WEEK 4+: CLEANUP

### Remove Feature Flags (After 5+ Days Stable)

#### Step 1: Update Code (Remove Conditional Logic)

```typescript
// BEFORE (with feature flags)
if (FEATURE_FLAGS.USE_OPTIMIZED_DAILY_REWARDS) {
  return await sendRemindersOptimized(users, today);
} else {
  return await sendRemindersLegacy(users, today);
}

// AFTER (optimized path only)
return await sendRemindersOptimized(users, today);
```

#### Step 2: Remove Legacy Code

```bash
# Find and remove legacy functions
grep -r "sendRemindersLegacy" src/
grep -r "unlockBadgeIndividual" src/

# Delete unused functions
# (Manually review and delete)
```

#### Step 3: Remove Feature Flag Config

```typescript
// BEFORE
export const FEATURE_FLAGS = {
  USE_OPTIMIZED_DAILY_REWARDS: process.env.USE_OPTIMIZED_DAILY_REWARDS === 'true',
  // ...
};

// AFTER (remove feature flags, keep concurrency/cache config)
export const CONCURRENCY_LIMITS = { /* ... */ };
export const CACHE_TTL = { /* ... */ };
```

#### Step 4: Update Environment

```bash
# Remove feature flag environment variables
unset USE_OPTIMIZED_DAILY_REWARDS
unset USE_OPTIMIZED_BADGE_UNLOCK

# Keep operational configs
# MATCH_ENRICHER_CONCURRENCY=10
# USE_REDIS_CACHE=true
```

---

### Document Lessons Learned

```markdown
# POST-P1 LESSONS LEARNED

## What Worked Well
1. Feature flags enabled safe gradual rollout
2. CONCURRENTLY pattern prevented production locks
3. Automated staging tests caught issues early
4. Conservative ‚Üí optimized approach minimized risk

## What Could Be Improved
1. Should have had performance budgets from day 1
2. Need observability before optimization (measure twice, optimize once)
3. Performance regression tests missing from CI

## Recommendations for Future
1. Enforce query budgets: Max 5 queries/endpoint, 10/job
2. Require EXPLAIN ANALYZE during code review
3. Add performance CI: Test query count, memory usage
4. Set up monitoring FIRST, optimize SECOND
```

---

## üìû SUPPORT & ESCALATION

### If Issues Occur

1. **Check Logs First**:
   ```bash
   tail -100 logs/error.log
   tail -100 logs/combined.log | grep ERROR
   ```

2. **Check Pool Stats**:
   ```bash
   curl -s http://localhost:3000/health/pool | jq
   ```

3. **Check Redis (if using PR-P1D)**:
   ```bash
   redis-cli ping
   redis-cli INFO stats
   ```

4. **Rollback if Necessary**:
   - Use emergency rollback procedure (30 seconds)
   - Notify team
   - Document issue for post-mortem

---

## üìà SUCCESS METRICS

### Performance Targets (After All PRs)

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Daily rewards queries | ‚â§3 | ___ | ___ |
| Badge unlock queries | ‚â§10 | ___ | ___ |
| Pool utilization | <50% | ___ | ___ |
| Standings P95 latency | <200ms | ___ | ___ |
| Cache hit rate (standings) | >80% | ___ | ___ |
| Migration table locks | 0s | ___ | ___ |

### Reliability Targets

- [ ] Zero pool exhaustion events
- [ ] Zero fire-and-forget errors
- [ ] Zero table locks during migrations
- [ ] Job error rate unchanged (<0.1%)
- [ ] No production incidents from optimizations

---

## ‚úÖ FINAL CHECKLIST

Before marking deployment complete:

- [ ] All 4 PRs deployed to production
- [ ] All success criteria met
- [ ] Performance metrics collected
- [ ] No production errors for 5+ days
- [ ] Documentation updated
- [ ] Team trained on new features
- [ ] Monitoring dashboards updated
- [ ] Stakeholders notified of success

---

**DEPLOYMENT STATUS**: Ready for Week 1 (PR-P1A + PR-P1B)

**NEXT ACTION**: Deploy PR-P1A indexes to production (Day 1, Monday)

---

**Last Updated**: 2026-02-02
**Related Documents**:
- `docs/POST-P1-FINAL-SUMMARY.md` - Overall implementation summary
- `docs/STAGING-TEST-PLAN.md` - Staging test procedures
- `docs/CACHING-IMPLEMENTATION-EXAMPLES.md` - Caching examples
- `docs/PR-P1A-MIGRATION-SAFETY.md` - PR-P1A details
- `docs/PR-P1B-N+1-ELIMINATION.md` - PR-P1B details
- `docs/PR-P1C-CONCURRENCY-CONTROL.md` - PR-P1C details
- `docs/PR-P1D-CACHING-INDEXES.md` - PR-P1D details

